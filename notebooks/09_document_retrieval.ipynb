{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Similarity for Retrieval\n",
    "\n",
    "Some topics to cover \n",
    "\n",
    "- Document retrieval in the real world: Recommending similar documents/products    \n",
    "- Bag of words features\n",
    "- Descriving text with tf-idf  **<font color='red'>(Exercise Solved)</font>**\n",
    "- Finding the nearest neighbor: naive approach.\n",
    "- KDtrees: fast (exact or approximate) search of similar items.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import nltk\n",
    "import re\n",
    "import sklearn\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2) Features for documents\n",
    "\n",
    "\n",
    "We will denote by\n",
    "\n",
    "- $W= \\{w_1, \\dots, w_D\\}$ the set of words used to make the representations.\n",
    "- $X$ our corpus of documents.\n",
    "- $X_w$ the set of documents that contain word $w$. \n",
    "\n",
    "### Bag of words vector  (or `tf` vector)\n",
    "\n",
    "\n",
    "- The bag of words representation for a document $x$ given a vocabulary $W$, or the term frequency vector **$\\text{tf}(X;W)$** is defined as \n",
    "\n",
    "$$\n",
    "\\text{tf}(x;W) = \\left( \\#\\{w_1| w_1 \\in x\\}, \\dots, \\#\\{w_D| w_D \\in x\\})\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### Term frequency Inverse Document frequency ( `tf * idf`)\n",
    "\n",
    "The objective of tf-idf representation is to emphasize the most relevant words of the documents. We want to emphasize:\n",
    "\n",
    "- Words that appear **frequently in the document**: term frequency \n",
    "- Words that appear **rarely in the corpus**: inverse document frequency\n",
    "\n",
    "#### Definition of the feature vectors\n",
    "\n",
    "\n",
    "- The **$\\text{tf}(X;W)$** vector for a document $x$ is defined as \n",
    "\n",
    "$$\n",
    "\\text{tf}(x;W) = \\left( \\#\\{w_1| w_1 \\in x\\}, \\dots, \\#\\{w_D| w_D \\in x\\})\\right)\n",
    "$$\n",
    "\n",
    "- The **$\\text{idf}(W; X)$** vector is defined as \n",
    "\n",
    "**$$\\text{idf}(W; X) = \\left( \\text{idf}(w_1; X), \\dots, \\text{idf}(w_D; X)\\right)$$** \n",
    "   \n",
    "$\\,\\,\\,\\,\\,\\,\\,$ A component of the feature for word $w \\in W$ in the corpus $X$ is defined as \n",
    "\n",
    "$$\n",
    "\\text{idf}(w, X) = log\\left(\\frac{|X|}{1+|X_{w}|}\\right)\n",
    "$$\n",
    "\n",
    "$\\,\\,\\,\\,\\,\\,\\,$Which simply means the full vector is \n",
    "$$\n",
    "\\text{idf}(w, X) = \\left( log\\left(\\frac{|X|}{1+|X_{w_1}|}\\right), \\dots, log\\left(\\frac{|X|}{1+|X_{w_D}|}\\right) \\right)\n",
    "$$\n",
    "\n",
    "- The tfidf vector for a document $x$ will be: $tf(x; X) * idf(X)$\n",
    "\n",
    "#### Observations\n",
    "\n",
    "- If a word appears in a few documents the idf vector will increase its weight.\n",
    "\n",
    "- If a word appears in a lots of documents documents the idf vector will decrease its weight.\n",
    "\n",
    "#### `sklearn.feature_extraction.text.TfidfVectorizer`\n",
    "\n",
    "- Notice that the implementation in sklearn already prevents zero divisions by default. This happens if `smooth_idf=True`.\n",
    "\n",
    "- By default the tfidf will only use words since `ngram_range=(1, 1)`. But this can be changed to allow n-grams in the feature vector components.\n",
    "\n",
    "#### Example\n",
    "\n",
    "Let us assume we have a corpus with one milion documents\n",
    "\n",
    "- Consider a word appearping in 100 documents:\n",
    "\n",
    "$$\\log\\left(\\frac{1000.000}{1 + 100} \\right) = 9.200$$\n",
    "\n",
    "- Consider a word appearing in 100.000 documents\n",
    "\n",
    "$$\\log\\left(\\frac{1000.000}{1 + 100.000} \\right) = 2.197$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make your own TfidfVectorizer\n",
    "\n",
    "\n",
    "#### Generate tf vector\n",
    "- check results are the same as sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_vocabulary(corpus, splitter):\n",
    "    vocabulary = set()\n",
    "    X_w = dict()\n",
    "    \n",
    "    for document in corpus:\n",
    "        words      = set(splitter.findall(document.lower()))\n",
    "        vocabulary = vocabulary.union(words)\n",
    "        for w in words:\n",
    "            X_w[w] = X_w.get(w, 0) + 1\n",
    "            \n",
    "    return vocabulary, X_w\n",
    "\n",
    "def term_frequency(document, word_to_ind, splitter, \n",
    "                   normalize=True, word_inds=False):\n",
    "    \n",
    "    words = splitter.findall(document.lower())\n",
    "    n_features = len(word_to_ind)\n",
    "    tf = sp.sparse.lil_matrix( (1, n_features), dtype=float)\n",
    "    \n",
    "    word_indices = []\n",
    "    for w in words:\n",
    "        word_indices.append(word_to_ind[w])\n",
    "        tf[0, word_to_ind[w]] += 1\n",
    "\n",
    "    if word_inds:\n",
    "        if normalize:\n",
    "            return tf.multiply(1/sp.sparse.linalg.norm(tf))\n",
    "        else:\n",
    "            return tf\n",
    "    else:\n",
    "        if normalize:\n",
    "            return tf.multiply(1/sp.sparse.linalg.norm(tf))\n",
    "        else:\n",
    "            return tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 1s, sys: 10.2 s, total: 1min 11s\n",
      "Wall time: 1min 13s\n"
     ]
    }
   ],
   "source": [
    "splitter = re.compile('(?u)\\\\b\\\\w\\\\w+\\\\b')\n",
    "%time vocabulary, X_w = build_vocabulary(newsgroups_train.data, splitter)\n",
    "\n",
    "word_to_ind = {v:i for i,v in enumerate(vocabulary)}\n",
    "ind_to_word = {v:k for k,v in word_to_ind.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.28 ms, sys: 1.87 ms, total: 7.15 ms\n",
      "Wall time: 6.94 ms\n"
     ]
    }
   ],
   "source": [
    "%time tf = term_frequency(newsgroups_train.data[0],\\\n",
    "                          word_to_ind, splitter, word_inds=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate tf from sklearn and compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.86 s, sys: 121 ms, total: 4.99 s\n",
      "Wall time: 5.07 s\n"
     ]
    }
   ],
   "source": [
    "tfidf_sk = sklearn.feature_extraction.text.TfidfVectorizer(use_idf=False,\n",
    "                                                           smooth_idf=False, \n",
    "                                                           sublinear_tf=False)\n",
    "\n",
    "%time tfidf_sk.fit(newsgroups_train.data)\n",
    "\n",
    "inverse_vocabulary_ = {v: k for k, v in tfidf_sk.vocabulary_.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.5 ms, sys: 1.67 ms, total: 4.17 ms\n",
      "Wall time: 3.25 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/david/anaconda/envs/py3/lib/python3.5/site-packages/sklearn/feature_extraction/text.py:1059: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "%time x_sk = tfidf_sk.transform([newsgroups_train.data[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparisson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isclose(tf.sum(), x_sk.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_x_own = [ind_to_word[k] for k in tf.nonzero()[1]]\n",
    "words_x_sk = [inverse_vocabulary_[k] for k in x_sk.nonzero()[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(words_x_own) == set(words_x_sk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate tfidf and compare with sklearn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_idf(X_w, word_to_ind, n_documents):\n",
    "\n",
    "    n_features = len(word_to_ind)\n",
    "    #idf = sp.sparse.csr_matrix( (1, n_features), dtype=float)\n",
    "    idf = np.zeros([1, n_features])\n",
    "    \n",
    "    for w in X_w:\n",
    "        idf[0, word_to_ind[w]] = np.log((1+n_documents)/(1 + X_w[w]))+1 \n",
    "        \n",
    "    #idf = idf + 1    \n",
    "    return sp.sparse.csr_matrix(idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lil_matrix is more efficient.\n",
    "tf = term_frequency(newsgroups_train.data[0], word_to_ind,\\\n",
    "                    splitter, normalize=False,word_inds=False)\n",
    "\n",
    "idf = compute_idf(X_w,word_to_ind, len(newsgroups_train.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9.640737377507692, 1.0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf.max(), idf.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_documents = len(X_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = tf.multiply(idf)\n",
    "tfidf = tfidf/sp.sparse.linalg.norm(tfidf)\n",
    "sp.sparse.linalg.norm(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = sklearn.feature_extraction.text.TfidfVectorizer()\n",
    "tfidf_vectorizer.fit(newsgroups_train.data);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/david/anaconda/envs/py3/lib/python3.5/site-packages/sklearn/feature_extraction/text.py:1059: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "tfidf_sklearn = tfidf_vectorizer.transform(newsgroups_train.data[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dtype('float64'), dtype('float64'))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.data.dtype, tfidf_sklearn.data.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.697815233022508 7.697815233022508\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(tfidf.sum(), tfidf_sklearn.sum())\n",
    "print(np.isclose(tfidf_sklearn.sum(),tfidf.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the most similar element toy example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def closest_point(all_points, query_point, dist):\n",
    "    closest_point_    = None\n",
    "    closest_distance_ = np.inf\n",
    "    \n",
    "    for current_point in all_points:\n",
    "        current_distance = dist(query_point, current_point)\n",
    "        \n",
    "        if  current_distance < closest_distance_:\n",
    "            closest_distance_ = current_distance\n",
    "            closest_point_    = current_point\n",
    "            \n",
    "    return closest_point_, closest_distance_\n",
    "\n",
    "def dist(x,y):\n",
    "    return np.sqrt(np.linalg.norm((x-y)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: [1 3]\n",
      "Closest to query: [0 3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1a11811ef0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxS1JhbG4JakwFrckFeb/AQzNEVyfkIdlAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a117f5a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "X = np.array([[1,1],[0,3],[4,3],[0,0],[1,5],[6,1],[-2,1],[4,4],[2,1],[-1,0]])\n",
    "x_components = [x[0] for x in X]\n",
    "y_components = [x[1] for x in X]\n",
    "query = np.array([1,3])\n",
    "\n",
    "closest_to_query, _ = closest_point(X, query, dist)\n",
    "print(\"Query: {}\\nClosest to query: {}\".format(query,closest_to_query))\n",
    "plt.scatter(x_components, y_components, color=\"black\")\n",
    "plt.scatter(query[0], query[1], color=\"red\")\n",
    "plt.scatter(closest_to_query[0], closest_to_query[1], color=\"blue\", linewidths=1, marker=\"o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the most similar element real data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closest point in numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import neighbors\n",
    "\n",
    "n_features = 40\n",
    "X = np.random.rand(1000000,n_features).astype(np.float32)\n",
    "x = np.random.rand(1,n_features).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 210 ms, sys: 96.7 ms, total: 307 ms\n",
      "Wall time: 255 ms\n",
      "CPU times: user 349 ms, sys: 84.6 ms, total: 433 ms\n",
      "Wall time: 226 ms\n",
      "\n",
      "closest row from x is 82844\n"
     ]
    }
   ],
   "source": [
    "%time distances =  np.mean((X-x)**2,1)\n",
    "#One second is too much\n",
    "%time closest = np.argmin(np.mean((X-x)**2,1))\n",
    "print(\"\\nclosest row from x is {}\".format(closest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = X[100,:] + np.random.rand()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom function for closest point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_features = 40\n",
    "X = np.random.rand(1000000,n_features).astype(np.float32)\n",
    "x = np.random.rand(1,n_features).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14 s, sys: 155 ms, total: 14.2 s\n",
      "Wall time: 14.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "closest_point_, closest_distance_ = closest_point(X, x, dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.28066936, 0.46672922, 0.44261822, 0.95386153, 0.36550063,\n",
       "       0.9916441 , 0.5349925 , 0.76487243, 0.43862563, 0.6367773 ,\n",
       "       0.51006705, 0.9160756 , 0.16399077, 0.42110866, 0.37581167,\n",
       "       0.34426594, 0.31198877, 0.5920748 , 0.6790636 , 0.89395803,\n",
       "       0.4599827 , 0.54770416, 0.8615159 , 0.14637236, 0.12648559,\n",
       "       0.5757881 , 0.48586133, 0.74784684, 0.2667317 , 0.99904555,\n",
       "       0.849827  , 0.26170462, 0.06040365, 0.84972125, 0.6484007 ,\n",
       "       0.35786378, 0.99717844, 0.32505742, 0.7922988 , 0.3157588 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closest_point_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.62816256"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closest_distance_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build kd trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.79 s, sys: 162 ms, total: 2.95 s\n",
      "Wall time: 3.01 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tree = sklearn.neighbors.KDTree(X, leaf_size=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 182 ms, sys: 1.63 ms, total: 184 ms\n",
      "Wall time: 185 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "distance_to_closest, closest_kdtree = tree.query(x, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "closest row from x is 996307\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nclosest row from x is {}\".format(closest_kdtree[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[996307]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closest_kdtree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that queries can have k to get the cloest k elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 170 ms, sys: 1.14 ms, total: 171 ms\n",
      "Wall time: 170 ms\n"
     ]
    }
   ],
   "source": [
    "%time distances_to_closest, close_kdtree = tree.query(x,k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[996307, 369132, 406663, 670786, 401563, 222748, 339689,  66945,\n",
       "        533785,  30510]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "close_kdtree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Fast search\n",
    "\n",
    "Doing NN search can be expensive. Given a query point $\\bf{x}$ we need to scan through each point $\\bf{x}^m$ in the dataset and compute the distance between $\\bf{x}$ and  $\\bf{x}^m$. That is:\n",
    "\n",
    "- $O(N)$ distance computations per 1-NN query \n",
    "- $O(N\\log(k))$ per k-NN query\n",
    "\n",
    "If N is huge this can be a problem.\n",
    "\n",
    "### KD Trees\n",
    "\n",
    "KD-trees are an efficient structure for efficiently representing our data. KD-trees provide an organization of our documents in terms of a certain partitioning of our space. The organization is based on recursively partitioning points into axis, defining \"boxes\".\n",
    "\n",
    "The KD-tree structure is based on making aligned cuts and maintaining lists of points that fall into each one of these different bins. This structure allows us  efficiently prune our search space so that we do not have to visit every single data point, for every query, necessarily. Sometimes we will have to do it but hopefully, in many cases, we will not have to do it.\n",
    "\n",
    "\n",
    "#### Using KD-trees\n",
    "\n",
    "Let us see how KD-trees can aid in efficiently making NN search. Let us assume we are given a KD_tree and let us see how to ue it. Later on we will see how to build the tree.\n",
    "\n",
    "Given a query point $\\bf{x}$:\n",
    "\n",
    "- Traverse the tree until the query point is reached. That is, check all the conditions of the KD-tree for the query point until a leave is reached.\n",
    "    - Once the query point is found save the \"box\" where it is found.\n",
    "    \n",
    "    \n",
    "- Compute the distance between each neighbor in the box and the query point.\n",
    "\n",
    "\n",
    "- Record the smallest distance to the NN so far.\n",
    "\n",
    "\n",
    "- Backtrack and try other branch at each node visited.\n",
    "    - Use the distance bound and bounding box of each node to prune parts of the three that cannot include the nearest neighbor.\n",
    "         \n",
    "         That is, **if the smallest distance is less than the distance from the query point to the bounding box there is no need to compute the distance between any point in the bounding box to the query point**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = np.random.rand(1000000,10).astype(np.float32)\n",
    "x = np.random.rand(1,10).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 119 ms, sys: 25.4 ms, total: 145 ms\n",
      "Wall time: 96 ms\n"
     ]
    }
   ],
   "source": [
    "%time distances =  np.mean((X-x)**2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 146 ms, sys: 11.9 ms, total: 158 ms\n",
      "Wall time: 80.7 ms\n",
      "\n",
      "closest row from x is 82844\n"
     ]
    }
   ],
   "source": [
    "# One second is too much\n",
    "%time closest_index = np.argmin(np.mean((X-x)**2,1))\n",
    "print(\"\\nclosest row from x is {}\".format(closest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make more efficient closest implementation\n",
    "\n",
    "- Why is this more efficient than the previous cell implementation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def closest_point(all_points, query_point, dist):\n",
    "    closest_point_    = None\n",
    "    closest_distance_ = np.inf\n",
    "    \n",
    "    for current_point in all_points:\n",
    "        current_distance = dist(query_point, current_point)\n",
    "        \n",
    "        if  current_distance < closest_distance_:\n",
    "            closest_distance_ = current_distance\n",
    "            closest_point_    = current_point\n",
    "            \n",
    "    return closest_point_, closest_distance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dist(x,y):\n",
    "    return np.sqrt(np.linalg.norm((x-y)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.4 s, sys: 227 ms, total: 13.6 s\n",
      "Wall time: 13.8 s\n",
      "\n",
      "closest row from x is (array([0.6088857 , 0.4209416 , 0.7173085 , 0.9602473 , 0.90554196,\n",
      "       0.9596096 , 0.9581918 , 0.6814053 , 0.6317129 , 0.8805297 ],\n",
      "      dtype=float32), 0.15431845)\n"
     ]
    }
   ],
   "source": [
    "#One second is too much\n",
    "%time closest = closest_point(X, x, dist)\n",
    "print(\"\\nclosest row from x is {}\".format(closest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.6088857 , 0.4209416 , 0.7173085 , 0.9602473 , 0.90554196,\n",
       "       0.9596096 , 0.9581918 , 0.6814053 , 0.6317129 , 0.8805297 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[closest_index, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build kdtree and use it for queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 922 ms, sys: 40.5 ms, total: 962 ms\n",
      "Wall time: 963 ms\n"
     ]
    }
   ],
   "source": [
    "%time tree = sklearn.neighbors.KDTree(X, leaf_size= 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.2 ms, sys: 1.67 ms, total: 22.9 ms\n",
      "Wall time: 21.8 ms\n"
     ]
    }
   ],
   "source": [
    "%time distance_to_closest, closest_kdtree = tree.query(x,k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "closest row from x is 381348\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nclosest row from x is {}\".format(closest_kdtree[0][0]))\n",
    "#distance_to_closest, closest_kdtree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that queries can have k to get the cloest k elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 51.1 ms, sys: 1.62 ms, total: 52.7 ms\n",
      "Wall time: 53.5 ms\n"
     ]
    }
   ],
   "source": [
    "%time distances_to_closest, close_kdtree = tree.query(x,k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[381348, 463316, 409224, 331076, 971061, 820789, 115824, 714320,\n",
       "        755725, 811501]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "close_kdtree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Complexity of NN search with KD-trees\n",
    "\n",
    "Complexity for a nearly balanced, binary tree\n",
    "\n",
    "- Construction complexity\n",
    "  - Size: 2N -1 nodes (if 1 datapoint at each leaf) O(N)\n",
    "  - Depth: O(log(N)) \n",
    "  - Median heuristic for split value + send points left,right: O(N) at every level of the tree\n",
    "  - Construction time: O(N log(N))\n",
    "- 1-NN query\n",
    "  - traverse down tree to starting point: O(log(N))\n",
    "  - Maximum backtrack and traverse: O(N) (N nodes in the worst case)\n",
    "  - Complexity range: O(log(N)) to O(N)\n",
    "\n",
    "Under some assumptions on the distribution of points we get O(log(N)) distance computations we need to do to compute a NN. Nevertheless the number of copmutations increases exponentially in d (the number of split dimensions the data has).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Timing closest to query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### How to build a KD-tree\n",
    "\n",
    "Start with a dataset with 2 features.\n",
    "\n",
    "- Pick a split dimension (feature)\n",
    "- Pick split value \n",
    "- Split the data for that split dimension and split value\n",
    "- Recurse on each of the groups\n",
    "\n",
    "The construction of the KD-tree will be based on storing, at each node in the tree, the \n",
    "\n",
    "- split dimension chosen\n",
    "- split value \n",
    "- bounding box that is as small as possible while containing points\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Choosing dimensions and splitting value\n",
    "\n",
    "People use heuristics to make splitting decisions.\n",
    "\n",
    "- Which dimension to do the split along ?\n",
    "  - Widest dimension\n",
    "  - Alternating dimensions\n",
    "- Which value to split at?\n",
    "  - Median value of the observations contained in the current box.\n",
    "  - Centerpoint value of the current box (ignoring the data in the box).\n",
    "- When to stop?\n",
    "  - Fewer than a given number m of data points left.\n",
    "  - A minimum width of the box is achieved.\n",
    "\n",
    "The heuristics can have big impact on the final data structure. See the following plot.\n",
    "\n",
    "![Alt Image Text](./images/heuristic_impact_kdtree.png  \"Optional Title\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
