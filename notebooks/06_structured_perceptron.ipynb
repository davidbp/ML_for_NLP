{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import scipy\n",
    "import numpy as np\n",
    "\n",
    "import os,sys,inspect\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0,parentdir) \n",
    "import skseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skseq.sequences import sequence\n",
    "from skseq.sequences.sequence import Sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting a Sequence Object\n",
    "\n",
    "- Sequence objects are defined in ``skseq/sequences/sequence.py``. \n",
    "    - A sequence in a supervised learning problem consist on a set of words and tags associated to words.\n",
    "    - For example ``w_1/t_1 w_2/t_2 w_3/t_3`` is a sequence of lenght 3 with words ``w_i`` and tags ``t_i``.\n",
    "\n",
    "\n",
    "In order to instanciate a Sequence we essentially need a list of words and a list of tags of the same size. In order to do it efficiently we will not store strings for the words and tags. We will store an integer values that will represent words and tags.\n",
    "\n",
    "- **.x** attribute: list of words (integer words)\n",
    "\n",
    "- **.y** attribute: list of tags (integer tags)\n",
    "\n",
    "Then we need to keep a mapping from integers to words and from integers to tags.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = Sequence(x=[\"my\",\"sequence\",\"is\",\"cool\"], y=[0,2,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "my/0 sequence/2 is/1 cool/1 "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my', 'sequence', 'is', 'cool']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 1, 1]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq.y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a vocabulary and a SequenceList\n",
    "\n",
    "Given a training set with words and tags we want to build a SequenceList object definded in  ``skseq/sequences/sequence_list.py``.\n",
    "\n",
    "A  SequenceList is a class that is initialized using a\n",
    "- dictionary for the words\n",
    "- a dictionary for the tags\n",
    "- an empty sequence list where the Sequences read from the data will be stored.\n",
    "\n",
    "\n",
    "    class SequenceList(object):\n",
    "\n",
    "        def __init__(self, x_dict, y_dict):\n",
    "            self.x_dict = x_dict\n",
    "            self.y_dict = y_dict\n",
    "            self.seq_list = []\n",
    "\n",
    "\n",
    "Let us create 3 sequence list for train, test and validation.  \n",
    "\n",
    "We will use the conll dataset and the class  ``PostagCorpus``.\n",
    "The class has a method ``.read_sequence_list_conll`` that will return the **SequenceList** object we want\n",
    "\n",
    "\n",
    "\n",
    "    def read_sequence_list_conll(self, train_file,\n",
    "                                 mapping_file=(\"%s/en-ptb.map\"\n",
    "                                               % dirname(__file__)),\n",
    "                                 max_sent_len=100000,\n",
    "                                 max_nr_sent=100000):\n",
    "\n",
    "        # Build mapping of postags:\n",
    "        mapping = {}\n",
    "        if mapping_file is not None:\n",
    "            for line in open(mapping_file):\n",
    "                coarse, fine = line.strip().split(\"\\t\")\n",
    "                mapping[coarse.lower()] = fine.lower()\n",
    "\n",
    "        instance_list = self.read_conll_instances(train_file,\n",
    "                                                  max_sent_len,\n",
    "                                                  max_nr_sent,\n",
    "                                                  mapping)\n",
    "\n",
    "        seq_list = SequenceList(self.word_dict, self.tag_dict)\n",
    "\n",
    "        for sent_x, sent_y in instance_list:\n",
    "            seq_list.add_sequence(sent_x, sent_y,  self.word_dict, self.tag_dict)\n",
    "\n",
    "        return seq_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skseq.readers.pos_corpus\n",
    "corpus = skseq.readers.pos_corpus.PostagCorpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01_string_basics.ipynb               04_Dense_word_representations.ipynb\r\n",
      "02_text_features.ipynb               05_word2vec_in_documents.ipynb\r\n",
      "03_hmm.ipynb                         06_structured_perceptron.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/data/conll\"\n",
    "\n",
    "data_path = parentdir + data_path\n",
    "\n",
    "train_seq = corpus.read_sequence_list_conll(data_path + \"/train-02-21.conll\", \n",
    "                                            max_sent_len=100, max_nr_sent=5000)\n",
    "\n",
    "test_seq = corpus.read_sequence_list_conll(data_path + \"/test-23.conll\",\n",
    "                                           max_sent_len=100, max_nr_sent=1000)\n",
    "\n",
    "dev_seq = corpus.read_sequence_list_conll(data_path + \"/dev-22.conll\", \n",
    "                                          max_sent_len=100, max_nr_sent=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'adp': 0,\n",
       " 'det': 1,\n",
       " 'noun': 2,\n",
       " 'num': 3,\n",
       " '.': 4,\n",
       " 'prt': 5,\n",
       " 'verb': 6,\n",
       " 'conj': 7,\n",
       " 'adv': 8,\n",
       " 'pron': 9,\n",
       " 'adj': 10,\n",
       " 'x': 11}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " corpus.tag_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'skseq.sequences.sequence_list.SequenceList'>\n",
      "<class 'skseq.sequences.sequence.Sequence'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train_seq))\n",
    "print(type(train_seq[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['x', 'y'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first sentence\n",
    "train_seq[0].__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42/2 40/2 43/6 44/2 41/4 "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_seq[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['x_dict', 'y_dict', 'seq_list'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_seq.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'adp': 0,\n",
       " 'det': 1,\n",
       " 'noun': 2,\n",
       " 'num': 3,\n",
       " '.': 4,\n",
       " 'prt': 5,\n",
       " 'verb': 6,\n",
       " 'conj': 7,\n",
       " 'adv': 8,\n",
       " 'pron': 9,\n",
       " 'adj': 10,\n",
       " 'x': 11}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set of possible tags Lambda\n",
    "# train_seq.y_dict is a dictionary of mappings from tag to integer id \n",
    "train_seq.y_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16937"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of possible words\n",
    "len(train_seq.x_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[42, 40, 43, 44, 41]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_seq[1].x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 2, 6, 2, 4]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_seq[1].y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 word:id pairs in the dicitionary\n",
      "\n",
      "In : 0\n",
      "an : 1\n",
      "Oct. : 2\n",
      "19 : 3\n",
      "review : 4\n"
     ]
    }
   ],
   "source": [
    "# Mapping from word to integer\n",
    "c =0\n",
    "print(\"First 5 word:id pairs in the dicitionary\\n\")\n",
    "for i in train_seq.x_dict:\n",
    "    print(i,\":\", train_seq.x_dict[i])\n",
    "    c +=1\n",
    "    if c>=5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using our corpus ``sequencelist`` to map integers to words\n",
    "\n",
    "Sequences can use ``SequenceList`` objects to map word_ids and tag_ids to words and tags.\n",
    "\n",
    "All ``sequence`` objects have the **``.to_words``** method which allows us to print the words given a **``SequenceList``** object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['x_dict', 'y_dict', 'seq_list'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_seq.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = train_seq[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ms./noun Haag/noun plays/verb Elianti/noun ./. '"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence.to_words(sequence_list=train_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to (linear) discriminative sequence models\n",
    "\n",
    "Discriminative sequence models aim to solve the following:\n",
    "\n",
    "$$\\underset{y\\,\\in\\,\\Lambda^N}{\\textrm{arg max}}\\ P(Y=y\\,|\\,X=x)=\\underset{y\\,\\in\\,\\Lambda^N}{\\textrm{arg max}}\\ \\boldsymbol{w}\\cdot\\boldsymbol{f}(x, y)$$\n",
    "\n",
    "where $\\boldsymbol{w}$ is the model's weight vector, and $\\boldsymbol{f}(x, y)$ is a feature vector. Notice that now both $y$ and $x$ are $N$-dimensional vectors, whereas in Day 1, these variables were just scalar numbers.\n",
    "\n",
    "In Day 2, sequences were scored using the log-probability. On today's models we are still scoring the sequences; the only difference is the scores are now computed as the product of the weights with the feature vector:\n",
    "\n",
    "\n",
    "| score | Hidden Markov Models| Discriminative Models  |\n",
    "| ------------------------------- | ---------------- | ---------------- |\n",
    "| $\\textrm{score}_\\textrm{emiss}$ | $\\log P(x_i\\,|\\,y_i) $ | $\\boldsymbol{w}\\cdot\\boldsymbol{f}_\\textrm{emiss}(i, x, y_i)$ |\n",
    "| $\\textrm{score}_\\textrm{init}$ | $\\log P(y_1\\,|\\,\\mathrm{start}) $ | $\\boldsymbol{w}\\cdot\\boldsymbol{f}_\\textrm{init}(x, y_1)$ |\n",
    "| $\\textrm{score}_\\textrm{trans}$ | $\\log P(y_{i+1}\\,|\\,y_i) $ | $\\boldsymbol{w}\\cdot\\boldsymbol{f}_\\textrm{trans}(i, x, y_i, y_{i+1})$ |\n",
    "| $\\textrm{score}_\\textrm{final}$ | $\\log P(\\mathrm{stop}\\,|\\,y_N) $ | $\\boldsymbol{w}\\cdot\\boldsymbol{f}_\\textrm{final}(x, y_N)$ |\n",
    "\n",
    "Notice that the scores computed using the feature vector depend on two sequential values of the output variable, $y$, but may depend on the whole observated input, $x$. We can now rewrite the above expression as\n",
    "\n",
    "$$\n",
    "\\underset{y\\,\\in\\,\\Lambda^N}{\\textrm{arg max}}\\ \n",
    "\\sum_{i=1}^N \\boldsymbol{w}\\cdot\\boldsymbol{f}_\\textrm{emiss}(i, x, y_i) + \n",
    "\\boldsymbol{w}\\cdot\\boldsymbol{f}_\\textrm{init}(x, y_1) + \n",
    "\\sum_{i=1}^{N-1}\\boldsymbol{w}\\cdot\\boldsymbol{f}_\\textrm{trans}(i, x, y_i, y_{i+1}) + \n",
    "\\boldsymbol{w}\\cdot\\boldsymbol{f}_\\textrm{final}(x, y_N) = \n",
    "\\\\\n",
    "\\underset{y\\,\\in\\,\\Lambda^N}{\\textrm{arg max}}\\ \n",
    "\\sum_{i=1}^N \\textrm{score}_\\textrm{emiss}(i, x, y_i) + \n",
    "\\textrm{score}_\\textrm{init}(x, y_1) + \n",
    "\\sum_{i=1}^{N-1}\\textrm{score}_\\textrm{trans}(i, x, y_i, y_{i+1}) +\n",
    "\\textrm{score}_\\textrm{final}(x, y_N)\n",
    "$$\n",
    "\n",
    "The reader can notice that feature vectors depend locally on the output variable. The features depend on \n",
    "\n",
    "- a single $y_i$ in the case of emission scores, initial scores and final scores.\n",
    "- or a pair  $y_i, y_{i+1}$ in the case of transition scores). \n",
    "\n",
    "### Features\n",
    "\n",
    "Today we will use two types of simple features. \n",
    "\n",
    "- **Features that mimic the features used by the HMM**\n",
    "    - This sill allow us to directly compare the performance of a generative vs a discriminative approach\n",
    "    \n",
    "\n",
    "- **Features that are implicit in the HMM** which are simple indicatiors of the initial, transition, final and emission events.\n",
    "    - Given a certain position $i$ and state $c$ the set of features that mimic the HMM are:\n",
    "\n",
    "\n",
    "| Conditions to be met       |    Name             |\n",
    "| ----------------           | ----------------    |\n",
    "| $y_i=c  \\,\\, \\& \\,\\, i =0$        | Initial features    |\n",
    "| $y_i=c   \\,\\, \\& \\,\\, y_{i-1}=c$  | Transition features |\n",
    "| $y_i=c_k \\,\\, \\& \\,\\,  i=N$        | Final features      |\n",
    "| $x_i=w_j \\,\\, \\& \\,\\,  y_i=c_k$    | Emission features   |\n",
    "\n",
    "When we used a generative model we were forced to make some independence assumptions. However, since we are now in a discriminative approach,where we model $P(Y | X)$ rather than $P(X,Y)$ we are not tied anymore to some of these assumptions. In particular:\n",
    "\n",
    "- We may use “overlapping” features, e.g., features that fire simultaneously for many instances. For example, we can use a feature for a word, such as a feature which fires for the word ”brilliantly”, and another for prefixes and suffixes of that word, such as one which fires if the last two letters of the word are ”ly”. This would lead to an awkward model if we wanted to insist on a generative approach.\n",
    "\n",
    "\n",
    "- We may use features that depend arbitrarily on the entire input sequence $x$. On the other hand, we still need to resort to “local” features with respect to the outputs (e.g. looking only at consecutive state pairs), otherwise decoding algorithms will become more expensive.\n",
    "\n",
    "\n",
    "#### Typical features used for POS taggigng with discriminative models\n",
    "\n",
    "The following table shows some typical POS tagging features. Let us consider $P_{set}$ and $S_{set}$ to be two sets of prefixes and sufixes respectively (set by the user).\n",
    "\n",
    "\n",
    "| Conditions to be met for some of the most typical POS features     |    Name      |\n",
    "| ----------------                                | ----------------    |\n",
    "| $y_i=c , \\,\\,  i =0$                      | Initial features    |\n",
    "| $y_i=c ,\\,\\,  y_{i-1}=c$                | Transition features |\n",
    "| $y_i=c_k ,\\,\\, i=N$                     | Final features      |\n",
    "| $x_i=w_j ,\\,\\,  y_i=c_k$                 | Basic Emission features|\n",
    "| $x_i=w_j ,\\,\\,  w_j \\text{ is uppercased } ,\\,\\,  y_i=c_k$                 | Upper case features|\n",
    "| $x_i=w_j ,\\,\\,  w_j \\text{ contains digit} ,\\,\\,  y_i=c_k$                 | Digit features|\n",
    "| $x_i=w_j ,\\,\\,  w_j \\text{ contains hyphen} ,\\,\\,  y_i=c_k$                 | Hypthen features|\n",
    "| $x_i=w_j ,\\,\\,  w_j[0:i] \\in P_{set}  \\forall i \\in \\{1,2,3\\}  ,\\,\\,  y_i=c_k$                 | Prefix features|\n",
    "| $x_i=w_j ,\\,\\,  w_j[-i] \\in S_{set}  \\forall i \\in \\{1,2,3\\}  ,\\,\\,  y_i=c_k$                 | Suffix features|\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "We can have more complex features which look arbitrarily to the input sequence. We are not going to have them in this exercise only for performance reasons (to have less features and smaller caches). State-of-the-art sequence classifiers can easily reach over one million features!\n",
    "\n",
    "Our features subdivide in two groups\n",
    "\n",
    "- **node features**: $f_{\\text{emiss}}, f_{\\text{init}}, f_{\\text{final}}$. Node features depend only on a single position in the state sequence (or node in the trellis).\n",
    "    \n",
    "    \n",
    "- **edge features**: $f_{\\text{trans}}$. Edge features depend on two consecutive positions in the state sequence (an edge in the trellis)\n",
    "\n",
    "\n",
    "    \n",
    "| score  definitions: scalar product between features and weights |\n",
    "| ------------------------------- | ---------------- |\n",
    "| $\\textrm{score}_\\textrm{emiss}\\,(i,x,y_1) =\\boldsymbol{w}\\cdot\\boldsymbol{f}_\\textrm{emiss}\\,(i, x, y_i)$ |\n",
    "|$\\textrm{score}_\\textrm{init}(x,y_1)=\\boldsymbol{w}\\cdot\\boldsymbol{f}_\\textrm{init} \\,(x, y_1)$ |\n",
    "| $\\textrm{score}_\\textrm{trans}\\,(i,x,y_i,y_{i+1}) = \\boldsymbol{w}\\cdot\\boldsymbol{f}_\\textrm{trans}\\,(i, x, y_i, y_{i+1})$ |\n",
    "| $\\textrm{score}_\\textrm{final}\\,(x,y_N) = \\boldsymbol{w}\\cdot\\boldsymbol{f}_\\textrm{final}\\,(x, y_N)$ |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Decoding\n",
    "\n",
    "One important thing to notice is that the decoding process - the process by which we pick the most likely label $y_i$ for the observation $x_i$ - stays the same as in an standard HMM. This means **we do not need to develop new decoders, as long as we have existing decorders for HMMs** only new functions to compute the scores. Because of this, we will keep using the Viterbi and Forward-Backward algorithms.\n",
    "\n",
    "\n",
    "For decoding,   there are three important problems that need to be solved:\n",
    "\n",
    "1. Given $X=x$, compute the most likely output sequence $\\hat{y}$ (the one which maximizes $P_{w}(Y=y|X=x)$). \n",
    "2. Compute the posterior marginals $P_{w}(Y_i=y_i|X=x)$ at each position $i$.\n",
    "3. Evaluate the partition function $Z(w,x)$. \n",
    "\n",
    "Interestingly, all these problems can be solved by using the very same\n",
    "algorithms that were \n",
    "already implemented for HMMs: the Viterbi algorithm (for 1) and the forward-backward algorithm (for 2--3). All that changes is the way the scores are computed. \n",
    "\n",
    "\n",
    "\n",
    "### Training the classifier\n",
    "\n",
    "Today we will cover two different approaches to training sequential discriminative models. Given a training set with $M$ observation-label pairs, $\\{(x_m, y_m)\\}_{m=1}^M$ (note that $x_m$, $y_m$ are $N$-dimensional vectors, as $m$ indexes the training sample):\n",
    "\n",
    "* **Structured Perceptron** iteratively updates $w$ in order to correctly classify the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training  a POS tagger in english\n",
    "\n",
    "## Loading the data: the conll dataset for part of speech tagging\n",
    "\n",
    "\n",
    "\n",
    "#### Example of two sequences in the dataset\n",
    "\n",
    "    1\tMs.\t_\tNN\tNNP\t_\t2\tNMOD\t_\t_\n",
    "    2\tHaag\t_\tNN\tNNP\t_\t3\tSUB\t_\t_\n",
    "    3\tplays\t_\tVB\tVBZ\t_\t0\tROOT\t_\t_\n",
    "    4\tElianti\t_\tNN\tNNP\t_\t3\tOBJ\t_\t_\n",
    "    5\t.\t_\t.\t.\t_\t3\tP\t_\t_\n",
    "\n",
    "    1\tRolls-Royce\t_\tNN\tNNP\t_\t4\tNMOD\t_\t_\n",
    "    2\tMotor\t_\tNN\tNNP\t_\t4\tNMOD\t_\t_\n",
    "    3\tCars\t_\tNN\tNNPS\t_\t4\tNMOD\t_\t_\n",
    "    4\tInc.\t_\tNN\tNNP\t_\t5\tSUB\t_\t_\n",
    "    5\tsaid\t_\tVB\tVBD\t_\t0\tROOT\t_\t_\n",
    "    6\tit\t_\tPR\tPRP\t_\t7\tSUB\t_\t_\n",
    "    7\texpects\t_\tVB\tVBZ\t_\t5\tVMOD\t_\t_\n",
    "    8\tits\t_\t.\tPRP$\t_\t10\tNMOD\t_\t_\n",
    "    9\tU.S.\t_\tNN\tNNP\t_\t10\tNMOD\t_\t_\n",
    "    10\tsales\t_\tNN\tNNS\t_\t12\tSUB\t_\t_\n",
    "    11\tto\t_\tTO\tTO\t_\t12\tVMOD\t_\t_\n",
    "    12\tremain\t_\tVB\tVB\t_\t7\tVMOD\t_\t_\n",
    "    13\tsteady\t_\tJJ\tJJ\t_\t12\tPRD\t_\t_\n",
    "    14\tat\t_\tIN\tIN\t_\t12\tVMOD\t_\t_\n",
    "    15\tabout\t_\tIN\tIN\t_\t17\tNMOD\t_\t_\n",
    "    16\t1,200\t_\tCD\tCD\t_\t15\tAMOD\t_\t_\n",
    "    17\tcars\t_\tNN\tNNS\t_\t14\tPMOD\t_\t_\n",
    "    18\tin\t_\tIN\tIN\t_\t12\tVMOD\t_\t_\n",
    "    19\t1990\t_\tCD\tCD\t_\t18\tPMOD\t_\t_\n",
    "    20\t.\t_\t.\t.\t_\t5\tP\t_\t_\n",
    "    \n",
    "\n",
    "#### corpus.read_sequence_list_conll\n",
    "\n",
    "This method will read the data. Each phrase in the dataset will become a **Sequence** which will be appended to the SequenceList\n",
    "\n",
    "skseq.sequences.sequence_list.SequenceList\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16937, 16937, 16937)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is completly unrealistic, words in train and test will be different usually\n",
    "len(train_seq.x_dict),len(test_seq.x_dict), len(dev_seq.x_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 12, 12)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_seq.y_dict),len(test_seq.y_dict), len(dev_seq.y_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Mapper (or feature template)\n",
    "\n",
    "Given a dataset, first we will build a \n",
    "\n",
    "-  **``SequenceList``** with the dataset.\n",
    "\n",
    "in order to build the features from the instanciated  **``SequenceList``** we will use\n",
    "\n",
    "- An instance from **``IDFeatures``** (we will call it feature_mapper) must be instanciated\n",
    "- **``feature_mapper.build_features()``** must be executed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating features from data\n",
    "\n",
    "\n",
    "\n",
    "### Instantiating a feature_mapper\n",
    "We will assume feature_mapper has been instantiated with\n",
    "\n",
    "    feature_mapper = skseq.sequences.id_feature.IDFeatures(train_seq)\n",
    "\n",
    "\n",
    "**The IDFeatures object will be referred to as a  ```feature_mapper```.**\n",
    "\n",
    "\n",
    "#### About feature_mappers\n",
    "A ```feature_mapper``` will contain the following attributes:\n",
    "\n",
    "- the dataset in ```.dataset```\n",
    "    - if we instantiate the feature mapper with a dataset X then ```feature_mapper.dataset```will be a copy of X\n",
    "\n",
    "\n",
    "- a dictionary of features in ```.feature_dict```\n",
    "    - this dictionary will default to ```{}```. \n",
    "    - In order to build the features the feature mapper must call ```.build_features()``` function.\n",
    "    \n",
    "    \n",
    "- a list of features in ```.feature_list```\n",
    "    - this list will default to ```[]```. \n",
    "    - In order to build the list of features the feature mapper must call ```.build_features()``` function.\n",
    "\n",
    "A ```feature_mapper``` will contain the method \n",
    "\n",
    "- A method to generate features, ```.build_features```\n",
    "    - this method will create features using the ```.dataset``.\n",
    "    - This method will also fill ```.feature_dict``` and ```.feature_list``\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_mapper = skseq.sequences.id_feature.IDFeatures(train_seq)\n",
    "#from skseq.sequences import extended_feature\n",
    "#feature_mapper = skseq.sequences.extended_feature.ExtendedFeatures(train_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_mapper.feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_mapper.feature_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling ```feature_mapper.build_features()```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get features\n",
    "feature_mapper.build_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['feature_dict',\n",
      " 'feature_list',\n",
      " 'add_features',\n",
      " 'dataset',\n",
      " 'node_feature_cache',\n",
      " 'initial_state_feature_cache',\n",
      " 'final_state_feature_cache',\n",
      " 'edge_feature_cache']\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pprint(list(feature_mapper.__dict__.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15377"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_mapper.feature_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_mapper.feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['init_tag:adp',\n",
       " 'id:In::adp',\n",
       " 'id:an::det',\n",
       " 'prev_tag:adp::det',\n",
       " 'id:Oct.::noun']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(feature_mapper.feature_dict)[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['init_tag:adp',\n",
       " 'id:In::adp',\n",
       " 'id:an::det',\n",
       " 'prev_tag:adp::det',\n",
       " 'id:Oct.::noun',\n",
       " 'prev_tag:det::noun',\n",
       " 'id:19::num',\n",
       " 'prev_tag:noun::num',\n",
       " 'id:review::noun',\n",
       " 'prev_tag:num::noun']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(feature_mapper.feature_dict)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'final_prev_tag', 'id', 'init_tag', 'prev_tag'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([x.split(\":\")[0] for x in feature_mapper.feature_dict.keys()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retreaving features from a `feature_mapper`\n",
    "\n",
    "we can get the features of a given sequence using **``feature_mapper.get_sequence_features``**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[69]], [[28], [36], [34], [18]], [[68]], [[70], [66], [71], [72], [67]]]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_mapper.feature_list[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting a feature mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15377, 5000)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_mapper.feature_dict),len(feature_mapper.feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for any position i, this will be len 4, corresponding to \n",
    "# initial,emission, transition, and final features\n",
    "m = 1\n",
    "len(feature_mapper.feature_list[m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_seq = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42/2 40/2 43/6 44/2 41/4 "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_mapper.dataset[id_seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lenght of the sequence\n",
    "len(feature_mapper.dataset[id_seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lenght of the types of features\n",
    "len(feature_mapper.feature_list[id_seq])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Understanding features in a feature mapper\n",
    "\n",
    "\n",
    "### What are all those numbers?\n",
    "\n",
    "Notice that ```feature_mapper.feature_list[id_seq]``` is a list of lists of length 4.\n",
    "\n",
    "This lenght is the same no matter ```id_seq```\n",
    "\n",
    "### Codification of the features\n",
    "\n",
    "All features are saved in **``feature_mapper.feature_dict``**.\n",
    "\n",
    "- **If it is our feature vector why it's not a vector? Good point! ** \n",
    "    - In order to make the algorithm fast, the code is written using dicts, so if we access only a few positions from the dict and compute substractions it will be much faster than computing the substraction of two huge weight vectors.\n",
    "\n",
    "Some features are identifyed by starting with **init_tag:**, **prev_tag:**,  **final_prev_tag:**, **id:**\n",
    "\n",
    "- **init_tag:** when they are Initial features\n",
    "    - Example: **``init_tag:noun``** is an initial feature that describes that the first word is a noun\n",
    "    \n",
    "    \n",
    "- **prev_tag:** when they are transition features\n",
    "    - Example: **``prev_tag:noun::noun``** is an transition feature that describes that the previous word was\n",
    "      a noun and the current word is a noun.\n",
    "    - Example: **``prev_tag:noun:.``** is an transition feature that describes that the previous word was\n",
    "      a noun and the current word is a `.` (this is usually foud as the last transition feature since most phrases will end up with a dot)\n",
    "      \n",
    "\n",
    "\n",
    "- **final_prev_tag:** when they are final features\n",
    "    - Example: **``final_prev_tag:.``** is a final feature stating that the last \"word\" in the sentence was a dot.\n",
    "\n",
    "\n",
    "- **id:** when they are emission features\n",
    "    - Example: **``id:plays::verb``** is an emission feature, describing that the current word is plays and the current hidden state is a verb.\n",
    "    - Example: **``id:Feb.::noun``** is an emission feature, describing that the current word is \"Feb.\" and the current hidden state is a noun.\n",
    "    \n",
    "Other features are identifyed by starting with **uppercased**, **suffix**, **preffix** etc...\n",
    "\n",
    "- **uppercased:** when they contain the current word with an Uppercase>\n",
    "    - Example: **``uppercased::noun``** is a feature stating that current word is upeprcased and the current tag is a noun.\n",
    "\n",
    "- **prefix:** when the current word contains a certain prefix.\n",
    "    - Example: prefix:Eli::noun\n",
    "\n",
    "- **suffix:** when the current word contains a certain suffix.\n",
    "    - Example: suffix:ing:verb\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial features: [[69]]\n",
      "Transition features: [[28], [36], [34], [18]]\n",
      "Final features: [[68]]\n",
      "Emission features: [[70], [66], [71], [72], [67]]\n"
     ]
    }
   ],
   "source": [
    "print (\"Initial features:\",     feature_mapper.feature_list[id_seq][0])\n",
    "print (\"Transition features:\",  feature_mapper.feature_list[id_seq][1])\n",
    "print (\"Final features:\",       feature_mapper.feature_list[id_seq][2])\n",
    "print (\"Emission features:\",    feature_mapper.feature_list[id_seq][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_feature_dict = {word: pos for pos, word in feature_mapper.feature_dict.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features\n",
    "\n",
    "features are coded as integers, using the inv_feature_dict we can see its interpretation\n",
    "\n",
    "Features have been assigned to a unique string that gives some hint on what they mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_seq = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The/det new/adj rate/noun will/verb be/verb payable/adj Feb./noun 15/num ./. '"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_seq[id_seq].to_words(train_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[98]],\n",
       " [[144], [105], [36], [148], [89], [105], [7], [97]],\n",
       " [[68]],\n",
       " [[14], [143], [145], [146], [147], [149], [150], [151], [67]]]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_mapper.feature_list[id_seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial features\n",
      "\t [98]\n",
      "\t\t init_tag:det\n",
      "\n",
      "\n",
      "Transition features\n",
      "\t [144]\n",
      "\t\t prev_tag:det::adj\n",
      "\t [105]\n",
      "\t\t prev_tag:adj::noun\n",
      "\t [36]\n",
      "\t\t prev_tag:noun::verb\n",
      "\t [148]\n",
      "\t\t prev_tag:verb::verb\n",
      "\t [89]\n",
      "\t\t prev_tag:verb::adj\n",
      "\t [105]\n",
      "\t\t prev_tag:adj::noun\n",
      "\t [7]\n",
      "\t\t prev_tag:noun::num\n",
      "\t [97]\n",
      "\t\t prev_tag:num::.\n",
      "\n",
      "\n",
      "Final features\n",
      "\t [68]\n",
      "\t\t final_prev_tag:.\n",
      "\n",
      "\n",
      "Emission features\n",
      "\t [14]\n",
      "\t\t id:The::det\n",
      "\t [143]\n",
      "\t\t id:new::adj\n",
      "\t [145]\n",
      "\t\t id:rate::noun\n",
      "\t [146]\n",
      "\t\t id:will::verb\n",
      "\t [147]\n",
      "\t\t id:be::verb\n",
      "\t [149]\n",
      "\t\t id:payable::adj\n",
      "\t [150]\n",
      "\t\t id:Feb.::noun\n",
      "\t [151]\n",
      "\t\t id:15::num\n",
      "\t [67]\n",
      "\t\t id:.::.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_type = [\"Initial features\", \"Transition features\", \"Final features\", \"Emission features\"]\n",
    "\n",
    "for feat,feat_ids in enumerate(feature_mapper.get_sequence_features(train_seq[id_seq])):\n",
    "    print(feature_type[feat])\n",
    "    for id_list in feat_ids:\n",
    "        print (\"\\t\",id_list)\n",
    "        for k,id_val in enumerate(id_list):\n",
    "            print (\"\\t\\t\", inv_feature_dict[id_val] )\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Given an input sequence, how to compute features that are activated\n",
    "\n",
    "We have stored all features in  **``feature_mapper.feature_dict``**.\n",
    "\n",
    "Now how can we know when a particular word and tag at a particular position fire any of the created features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 features in the dicitionary\n",
      "\n",
      "init_tag:adp : 0\n",
      "id:In::adp : 1\n",
      "id:an::det : 2\n",
      "prev_tag:adp::det : 3\n",
      "id:Oct.::noun : 4\n"
     ]
    }
   ],
   "source": [
    "# Looking at some features and the position they have assigned\n",
    "c =0\n",
    "print(\"First 5 features in the dicitionary\\n\")\n",
    "for i in feature_mapper.feature_dict:\n",
    "    print(i, \":\", feature_mapper.feature_dict[i])\n",
    "    c +=1\n",
    "    if c>=5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42/2 40/2 43/6 44/2 41/4 "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activated Features\n",
    "\n",
    "inside the feature mapper there is the **``get_initial_features``** method\n",
    "\n",
    "    def get_initial_features(self, sequence, y):\n",
    "        if y not in self.initial_state_feature_cache:\n",
    "            edge_idx = []\n",
    "            edge_idx = self.add_initial_features(sequence, y, edge_idx)\n",
    "            self.initial_state_feature_cache[y] = edge_idx\n",
    "        return self.initial_state_feature_cache[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42/2 40/2 43/6 44/2 41/4 "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_mapper.get_initial_features(sequence, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[373]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_mapper.get_transition_features(sequence, pos=1, y=1, y_prev=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[141]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_mapper.get_transition_features(sequence, pos=1, y=1, y_prev=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1695]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_mapper.get_transition_features(sequence, pos=1, y=1, y_prev=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'prev_tag:num::det'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_feature_dict[1695]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Structured perceptron\n",
    "\n",
    "In order to train a structured perceptron we need to construct a feature mapper that will translate Sequence objects to numerical features. Then the structured perceptron can be instanciated using\n",
    "\n",
    "- The corpus dictionary of words\n",
    "- The corpus dictionary of tags\n",
    "- The feature mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_mapper = skseq.sequences.id_feature.IDFeatures(train_seq)\n",
    "feature_mapper.build_features()\n",
    "\n",
    "#import skseq.sequences.extended_feature as exfc\n",
    "#feature_mapper = exfc.ExtendedFeatures(train_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.sequence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'adp': 0,\n",
       " 'det': 1,\n",
       " 'noun': 2,\n",
       " 'num': 3,\n",
       " '.': 4,\n",
       " 'prt': 5,\n",
       " 'verb': 6,\n",
       " 'conj': 7,\n",
       " 'adv': 8,\n",
       " 'pron': 9,\n",
       " 'adj': 10,\n",
       " 'x': 11}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.tag_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skseq.sequences.structured_perceptron as spc\n",
    "\n",
    "sp = spc.StructuredPerceptron(corpus.word_dict, corpus.tag_dict, feature_mapper)\n",
    "sp.num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'adp': 0,\n",
       " 'det': 1,\n",
       " 'noun': 2,\n",
       " 'num': 3,\n",
       " '.': 4,\n",
       " 'prt': 5,\n",
       " 'verb': 6,\n",
       " 'conj': 7,\n",
       " 'adv': 8,\n",
       " 'pron': 9,\n",
       " 'adj': 10,\n",
       " 'x': 11}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.state_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 16937)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.get_num_states(), sp.get_num_observations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15377"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_mapper.get_num_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### About the weights of the perceptron\n",
    "\n",
    "The perceptron starts with all weights set to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15377"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sp.parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.parameters.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions made by the structured Perceptron\n",
    "\n",
    "We can use the method **``.viterbi_decode``** from the structured perceptron to generate a sequence of predictions for a given sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = train_seq[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7/1 61/2 62/2 63/2 64/10 65/2 66/6 67/3 59/2 21/0 19/1 53/2 "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.get_num_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7/0 61/0 62/0 63/0 64/0 65/0 66/0 67/0 59/0 21/0 19/0 53/0 , 0.0)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.viterbi_decode(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a structured perceptron\n",
    "\n",
    "\n",
    "In order to train a structured perceptron we need to construct a feature mapper that will translate Sequence objects to numerical features. Then the structured perceptron can be instanciated using\n",
    "\n",
    "- The corpus dictionary of words\n",
    "- The corpus dictionary of tags\n",
    "- The feature mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_mapper = skseq.sequences.id_feature.IDFeatures(train_seq)\n",
    "feature_mapper.build_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spc.StructuredPerceptron(corpus.word_dict, corpus.tag_dict, feature_mapper)\n",
    "sp.num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 16937)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.get_num_states(), sp.get_num_observations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy before training\n",
    "\n",
    "We can use the methods\n",
    "\n",
    "- **``viterbi_decode_corpus``** to generate a list of sequences containing the predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_corpus(sequences, sequences_predictions):\n",
    "    \"\"\"Evaluate classification accuracy at corpus level, comparing with\n",
    "    gold standard.\"\"\"\n",
    "    total = 0.0\n",
    "    correct = 0.0\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        pred = sequences_predictions[i]\n",
    "        for j, y_hat in enumerate(pred.y):\n",
    "            if sequence.y[j] == y_hat:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions for the various sequences using the trained model.\n",
    "pred_train = sp.viterbi_decode_corpus(train_seq)\n",
    "pred_dev = sp.viterbi_decode_corpus(dev_seq)\n",
    "pred_test = sp.viterbi_decode_corpus(test_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SP -  Accuracy Train: 0.103 Dev: 0.100 Test: 0.105\n"
     ]
    }
   ],
   "source": [
    "# Evaluate and print accuracies\n",
    "eval_train = evaluate_corpus(train_seq.seq_list, pred_train)\n",
    "eval_dev = evaluate_corpus(dev_seq.seq_list, pred_dev)\n",
    "eval_test = evaluate_corpus(test_seq.seq_list, pred_test)\n",
    "print(\"SP -  Accuracy Train: %.3f Dev: %.3f Test: %.3f\"%(eval_train,eval_dev, eval_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the structured perceptron\n",
    "\n",
    "\n",
    "The structured perceptron has the method **train_supervised** which allow us to train the weights of the model\n",
    "\n",
    "- **train_supervised**: recieves a **SequenceList** object\n",
    "\n",
    "Let us recall that a **SequenceList**  is a list of **Sequence** objects.\n",
    "\n",
    "- Each **Sequence** has the **.x** and **.y** atribute which are the words and tags of the **Sequence** respectively.\n",
    "\n",
    "    - For example, an example of **Sequence** in our train data, train_seq[1], is \n",
    "        - Ms./noun Haag/noun plays/verb Elianti/noun ./. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<skseq.sequences.structured_perceptron.StructuredPerceptron at 0x1169dd400>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Accuracy: 0.822854\n",
      "Epoch: 1 Accuracy: 0.904985\n",
      "Epoch: 2 Accuracy: 0.925024\n",
      "Epoch: 3 Accuracy: 0.937884\n",
      "Epoch: 4 Accuracy: 0.943772\n",
      "CPU times: user 1min 45s, sys: 366 ms, total: 1min 45s\n",
      "Wall time: 1min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_epochs = 5\n",
    "sp.fit(feature_mapper.dataset, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving model weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15377"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sp.parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.8,  7.2, 11.4, ...,  0.6,  0. ,  0.2])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.save_model(\"perceptron_5_iter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp2 = spc.StructuredPerceptron(corpus.word_dict, corpus.tag_dict, feature_mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp2.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp2.load_model(dir=\"perceptron_5_iter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.8,  7.2, 11.4, ...,  0.6,  0. ,  0.2])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp2.parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating model quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions for the various sequences using the trained model.\n",
    "pred_train = sp.viterbi_decode_corpus(train_seq)\n",
    "pred_dev = sp.viterbi_decode_corpus(dev_seq)\n",
    "pred_test = sp.viterbi_decode_corpus(test_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SP -  Accuracy Train: 0.927 Dev: 0.893 Test: 0.902\n"
     ]
    }
   ],
   "source": [
    "# Evaluate and print accuracies\n",
    "eval_train = evaluate_corpus(train_seq.seq_list, pred_train)\n",
    "eval_dev = evaluate_corpus(dev_seq.seq_list, pred_dev)\n",
    "eval_test = evaluate_corpus(test_seq.seq_list, pred_test)\n",
    "print(\"SP -  Accuracy Train: %.3f Dev: %.3f Test: %.3f\"%(eval_train,eval_dev, eval_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the structured perceptron\n",
    "\n",
    "In order to make a tag prediction for a given sequence we can use the **``.viterbi_decode``** method\n",
    "\n",
    "### About  **``.viterbi_decode``**\n",
    "\n",
    "- Compute scores given the observation sequence\n",
    "- Run the forward algorithm and therefore gets\n",
    "    - the predicted sequence of states **``best_states``**\n",
    "    - the total score\n",
    "- Creates a new sequence named **``predicted_sequence``**\n",
    "    - copyes the sequence of words from the input\n",
    "    - assigns the tags from  **``best_states``** to the created sequence\n",
    "    \n",
    "Returns **``predicted_sequence``** as well sas the **``total_score``**\n",
    "    \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = train_seq[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45/2 46/2 47/2 48/2 49/6 50/9 51/6 52/9 53/2 54/2 38/5 55/6 56/10 10/0 57/0 58/3 59/2 21/0 60/3 41/4 "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rolls-Royce/noun Motor/noun Cars/noun Inc./noun said/verb it/pron expects/verb its/pron U.S./noun sales/noun to/prt remain/verb steady/adj at/adp about/adp 1,200/num cars/noun in/adp 1990/num ./. '"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence.to_words(train_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45/2 46/2 47/2 48/2 49/6 50/9 51/6 52/9 53/2 54/2 38/5 55/6 56/10 10/0 57/8 58/3 59/2 21/0 60/3 41/4 , 199.20000000000002)\n"
     ]
    }
   ],
   "source": [
    "aux = sp.viterbi_decode(sequence)\n",
    "print(aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rolls-Royce/noun Motor/noun Cars/noun Inc./noun said/verb it/pron expects/verb its/pron U.S./noun sales/noun to/prt remain/verb steady/adj at/adp about/adv 1,200/num cars/noun in/adp 1990/num ./. '"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sequence containing the original words and the tag prediction\n",
    "aux[0].to_words(train_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict a given unseen string sequence\n",
    "\n",
    "Let us assume we have the phrase \"David had been asked to write a challenging program for Angel .\"\n",
    "\n",
    "We have to:\n",
    "\n",
    "- convert our string to a ``vlex_seq2.sequences.sequence.Sequence`` object.\n",
    "- use the ``perceptron.viterbi_decode`` method with the previously build ``Sequence`` object\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = \"David had been asked to write a challenging program for Maria .\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1613, 271, 106, 2552, 38, 394, 92, 9404, 4140, 78, 4036, 41]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_ids  = [train_seq.x_dict[w] for w in p.split()]\n",
    "word_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1613/0 271/0 106/0 2552/0 38/0 394/0 92/0 9404/0 4140/0 78/0 4036/0 41/0 "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = skseq.sequences.sequence.Sequence(x=word_ids, y=[0 for w in word_ids])\n",
    "seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'David/noun had/verb been/verb asked/verb to/prt write/verb a/det challenging/noun program/noun for/adp Maria/noun ./. '"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.viterbi_decode(seq)[0].to_words(train_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[0]],\n",
       " [[92], [92], [92], [92], [92], [92], [92], [92], [92], [92], [92]],\n",
       " [[2995]],\n",
       " [[], [], [], [], [], [], [], [], [], [121], [], []])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The following features correspond to the activated features for the given \n",
    "# list of tags and list of words\n",
    "feature_mapper.get_sequence_features(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unseen words in the corpus\n",
    "An obvious question that might arise is **what happens when a word is not in x_dict** ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 16937 words in the dictionary\n"
     ]
    }
   ],
   "source": [
    "print(\"there are\", len(train_seq.x_dict), \"words in the dictionary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = \"David had been asked to write a challenging program for Angel .\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Angel'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-102-c2ef1abec888>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# this should fail because \"Angel\" was never seen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mword_ids\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-102-c2ef1abec888>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# this should fail because \"Angel\" was never seen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mword_ids\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'Angel'"
     ]
    }
   ],
   "source": [
    "# this should fail because \"Angel\" was never seen\n",
    "word_ids  = [train_seq.x_dict[w] for w in p.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_seq = skseq.sequences.sequence.Sequence(x=p.split(), y=[int(0) for w in p.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "David/0 had/0 been/0 asked/0 to/0 write/0 a/0 challenging/0 program/0 for/0 Angel/0 ./0 "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[0]],\n",
       " [[92], [92], [92], [92], [92], [92], [92], [92], [92], [92], [92]],\n",
       " [[2995]],\n",
       " [[], [], [], [], [], [], [], [], [], [121], [], []])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_mapper.get_sequence_features(new_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'David/noun had/verb been/verb asked/verb to/prt write/verb a/det challenging/noun program/noun for/adp Angel/noun ./. '"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.viterbi_decode(new_seq)[0].to_words(train_seq,\n",
    "                                       only_tag_translation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = \"Sara had been asked to write a challenging program for Angel .\"\n",
    "new_seq = skseq.sequences.sequence.Sequence(x=p.split(), y=[int(0) for w in p.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Sara/6 had/6 been/6 asked/6 to/5 write/6 a/1 challenging/2 program/2 for/0 Angel/2 ./4 ,\n",
       " 131.4)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.viterbi_decode(new_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sara/verb had/verb been/verb asked/verb to/prt write/verb a/det challenging/noun program/noun for/adp Angel/noun ./. '"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.viterbi_decode(new_seq)[0].to_words(train_seq,\n",
    "                                       only_tag_translation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercices: Adding new features\n",
    "\n",
    "Iniside skseq.sequences.extended_feature.py you will find\n",
    "\n",
    "```\n",
    "class ExtendedFeatures(IDFeatures):\n",
    "```\n",
    "\n",
    "Expand the function `add_emission_features(self, sequence, pos, y, features)` adding new features.\n",
    "\n",
    "One possible feature is already added and is the `hyphen::tag` feature that fires when a word contains a \"-\" inside it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
